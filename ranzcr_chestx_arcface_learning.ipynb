{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This is a densenet121 arcface model trained with cross entropy loss to assist image retrieval task. Some key points:\n",
    "- Does not have a validation set\n",
    "- Trained on the full chestx dataset\n",
    "- Will be used to generate features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_id = 0\n",
    "\n",
    "image_size = 512\n",
    "seed = 42\n",
    "warmup_epo = 1\n",
    "init_lr = 1e-3 \n",
    "batch_size = 28\n",
    "n_epochs = 25\n",
    "warmup_factor = 10\n",
    "num_workers = 24\n",
    "\n",
    "add_ext = False\n",
    "use_amp = False\n",
    "save = True\n",
    "debug = False\n",
    "use_neptune = False\n",
    "accumulation_step = 1\n",
    "device_id = 0 # your GPU_id\n",
    "\n",
    "kernel_type = 'metric_learning_ranzcr_chestx'\n",
    "data_dir = 'train'\n",
    "enet_type = 'densenet121'\n",
    "model_dir = f'weights/{kernel_type}'\n",
    "experiment_name = f'fold{fold_id}_{kernel_type}_{enet_type}'\n",
    "\n",
    "! mkdir $model_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from efficientnet_pytorch import model as enet\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import cv2\n",
    "import PIL.Image\n",
    "import random\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from warmup_scheduler import GradualWarmupScheduler\n",
    "import albumentations\n",
    "import geffnet\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import timm\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "from warnings import filterwarnings\n",
    "import neptune\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import math\n",
    "import glob\n",
    "filterwarnings(\"ignore\")\n",
    "\n",
    "device = torch.device('cuda') \n",
    "torch.cuda.set_device(device_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradualWarmupSchedulerV2(GradualWarmupScheduler):\n",
    "    def __init__(self, optimizer, multiplier, total_epoch, after_scheduler=None):\n",
    "        super(GradualWarmupSchedulerV2, self).__init__(optimizer, multiplier, total_epoch, after_scheduler)\n",
    "    def get_lr(self):\n",
    "        if self.last_epoch > self.total_epoch:\n",
    "            if self.after_scheduler:\n",
    "                if not self.finished:\n",
    "                    self.after_scheduler.base_lrs = [base_lr * self.multiplier for base_lr in self.base_lrs]\n",
    "                    self.finished = True\n",
    "                return self.after_scheduler.get_lr()\n",
    "            return [base_lr * self.multiplier for base_lr in self.base_lrs]\n",
    "        if self.multiplier == 1.0:\n",
    "            return [base_lr * (float(self.last_epoch) / self.total_epoch) for base_lr in self.base_lrs]\n",
    "        else:\n",
    "            return [base_lr * ((self.multiplier - 1.) * self.last_epoch / self.total_epoch + 1.) for base_lr in self.base_lrs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    print(f'Setting all seeds to be {seed} to reproduce...')\n",
    "seed_everything(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_training_set = pd.read_csv('chestx/Data_Entry_2017.csv')\n",
    "my_training_set['file_path'] = sorted(glob.glob('chestx/images_*/*/*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "my_training_set['Patient ID'] = le.fit_transform(my_training_set['Patient ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(random_state=42, shuffle=True)\n",
    "for fold, (_, valid_idx) in enumerate(skf.split(my_training_set, my_training_set['Patient ID'])):\n",
    "    my_training_set.loc[valid_idx, 'fold'] = fold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RANZERDataset(Dataset):\n",
    "    def __init__(self, df, mode, transform=None):\n",
    "        \n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.mode = mode\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        row = self.df.loc[index]\n",
    "        img = cv2.imread(row.file_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            res = self.transform(image=img)\n",
    "            img = res['image']\n",
    "                \n",
    "        img = img.astype(np.float32)\n",
    "        img = img.transpose(2,0,1)\n",
    "        label = torch.tensor(row['Patient ID']).float()\n",
    "        \n",
    "        if self.mode == 'test':\n",
    "            return torch.tensor(img).float()\n",
    "        else:\n",
    "            return torch.tensor(img).float(), label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms_train = albumentations.Compose([\n",
    "    albumentations.Resize(image_size, image_size),\n",
    "    albumentations.HorizontalFlip(p=0.5),\n",
    "    albumentations.ShiftScaleRotate(p=0.5, shift_limit=0.0625, scale_limit=0.2, rotate_limit=20),\n",
    "    albumentations.Cutout(p=0.5, max_h_size=16, max_w_size=16, fill_value=(0., 0., 0.), num_holes=16),\n",
    "    albumentations.Normalize(),\n",
    "])\n",
    "\n",
    "transforms_valid = albumentations.Compose([\n",
    "    albumentations.Resize(image_size, image_size),\n",
    "    albumentations.Normalize()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArcModule(nn.Module):\n",
    "    def __init__(self, in_features, out_features, s=10, m=0):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.s = s\n",
    "        self.m = m\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n",
    "        nn.init.xavier_normal_(self.weight)\n",
    "\n",
    "        self.cos_m = math.cos(m)\n",
    "        self.sin_m = math.sin(m)\n",
    "        self.th = torch.tensor(math.cos(math.pi - m))\n",
    "        self.mm = torch.tensor(math.sin(math.pi - m) * m)\n",
    "\n",
    "    def forward(self, inputs, labels):\n",
    "        cos_th = F.linear(inputs, F.normalize(self.weight))\n",
    "        cos_th = cos_th.clamp(-1, 1)\n",
    "        sin_th = torch.sqrt(1.0 - torch.pow(cos_th, 2))\n",
    "        cos_th_m = cos_th * self.cos_m - sin_th * self.sin_m\n",
    "        cos_th_m = torch.where(cos_th > self.th, cos_th_m, cos_th - self.mm)\n",
    "\n",
    "        cond_v = cos_th - self.th\n",
    "        cond = cond_v <= 0\n",
    "        cos_th_m[cond] = (cos_th - self.mm)[cond]\n",
    "\n",
    "        if labels.dim() == 1:\n",
    "            labels = labels.unsqueeze(-1)\n",
    "        onehot = torch.zeros(cos_th.size()).cuda()\n",
    "        labels = labels.type(torch.LongTensor).cuda()\n",
    "        onehot.scatter_(1, labels, 1.0)\n",
    "        outputs = onehot * cos_th_m + (1.0 - onehot) * cos_th\n",
    "        outputs = outputs * self.s\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricLearningModel(nn.Module):\n",
    "\n",
    "    def __init__(self, channel_size, out_feature, dropout=0.5, backbone='densenet121', pretrained=True):\n",
    "        super(MetricLearningModel, self).__init__()\n",
    "        self.backbone = timm.create_model(backbone, pretrained=pretrained)\n",
    "        self.channel_size = channel_size\n",
    "        self.out_feature = out_feature\n",
    "        self.in_features = self.backbone.classifier.in_features\n",
    "        self.margin = ArcModule(in_features=self.channel_size, out_features = self.out_feature)\n",
    "        self.bn1 = nn.BatchNorm2d(self.in_features)\n",
    "        self.dropout = nn.Dropout2d(dropout, inplace=True)\n",
    "        self.fc1 = nn.Linear(self.in_features * 16 * 16 , self.channel_size)\n",
    "        self.bn2 = nn.BatchNorm1d(self.channel_size)\n",
    "        \n",
    "    def forward(self, x, labels=None):\n",
    "        features = self.backbone.features(x)\n",
    "        features = self.bn1(features)\n",
    "        features = self.dropout(features)\n",
    "        features = features.view(features.size(0), -1)\n",
    "        features = self.fc1(features)\n",
    "        features = self.bn2(features)\n",
    "        features = F.normalize(features)\n",
    "        if labels is not None:\n",
    "            return self.margin(features, labels)\n",
    "        return features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Valid loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_func(train_loader):\n",
    "    model.train()\n",
    "    bar = tqdm(train_loader)\n",
    "    if use_amp:\n",
    "        scaler = torch.cuda.amp.GradScaler()\n",
    "    losses = []\n",
    "    for batch_idx, (images, targets) in enumerate(bar):\n",
    "\n",
    "        images, targets = images.to(device), targets.to(device).long()\n",
    "\n",
    "        if use_amp:\n",
    "            with torch.cuda.amp.autocast():\n",
    "                logits = model(images, targets)\n",
    "                loss = criterion(logits, targets)\n",
    "            scaler.scale(loss).backward()\n",
    "            if ((batch_idx + 1) %  accumulation_step == 0) or ((batch_idx + 1) == len(train_loader)):\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "        else:\n",
    "            logits = model(images, targets)\n",
    "            loss = criterion(logits, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        smooth_loss = np.mean(losses[-30:])\n",
    "\n",
    "        bar.set_description(f'loss: {loss.item():.5f}, smth: {smooth_loss:.5f}')\n",
    "\n",
    "    loss_train = np.mean(losses)\n",
    "    return loss_train\n",
    "\n",
    "\n",
    "def valid_func(valid_loader):\n",
    "    model.eval()\n",
    "    bar = tqdm(valid_loader)\n",
    "\n",
    "    PROB = []\n",
    "    TARGETS = []\n",
    "    losses1 = []\n",
    "    PREDS = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, targets) in enumerate(bar):\n",
    "\n",
    "            images, targets = images.to(device), targets.to(device).long()\n",
    "\n",
    "            logits = model(images, targets)\n",
    "\n",
    "            PREDS += [torch.argmax(logits, 1).detach().cpu()]\n",
    "            TARGETS += [targets.detach().cpu()]\n",
    "\n",
    "            loss1 = criterion(logits, targets)\n",
    "            losses1.append(loss1.item())\n",
    "           \n",
    "            bar.set_description(f'loss1: {loss1.item():.5f}')\n",
    "\n",
    "    PREDS = torch.cat(PREDS).cpu().numpy()\n",
    "    TARGETS = torch.cat(TARGETS).cpu().numpy()\n",
    "    accuracy = (PREDS==TARGETS).mean()\n",
    "    \n",
    "    loss_valid1 = np.mean(losses1)\n",
    "    return loss_valid1, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MetricLearningModel(512, my_training_set['Patient ID'].nunique())\n",
    "model.to(device);\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=init_lr/warmup_factor)\n",
    "scheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, n_epochs)\n",
    "scheduler_warmup = GradualWarmupSchedulerV2(optimizer, multiplier=10, total_epoch=warmup_epo, after_scheduler=scheduler_cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_this = my_training_set\n",
    "df_valid_this = my_training_set[my_training_set['fold'] == fold_id]\n",
    "\n",
    "dataset_train = RANZERDataset(df_train_this, 'train', transform=transforms_train)\n",
    "dataset_valid = RANZERDataset(df_valid_this, 'valid', transform=transforms_valid)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size, shuffle=True,  num_workers=num_workers, pin_memory=False)\n",
    "valid_loader = torch.utils.data.DataLoader(dataset_valid, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = {}\n",
    "accuracy_max = 0.\n",
    "\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    scheduler_warmup.step(epoch-1)\n",
    "    loss_train = train_func(train_loader)\n",
    "    loss_valid, accuracy = valid_func(valid_loader)\n",
    "\n",
    "    log['loss_train'] = log.get('loss_train', []) + [loss_train]\n",
    "    log['loss_valid'] = log.get('loss_valid', []) + [loss_valid]\n",
    "    log['lr'] = log.get('lr', []) + [optimizer.param_groups[0][\"lr\"]]\n",
    "    log['accuracy'] = log.get('accuracy', []) + [accuracy]\n",
    "    if use_neptune:\n",
    "        neptune.log_metric('training_loss', loss_train)\n",
    "        neptune.log_metric('validation_loss', loss_valid)\n",
    "        neptune.log_metric('accuracy', accuracy)\n",
    "    content = time.ctime() + ' ' + f'Fold {fold_id}, Epoch {epoch}, lr: {optimizer.param_groups[0][\"lr\"]:.7f}, loss_train: {loss_train:.5f}, loss_valid: {loss_valid:.5f}, accuracy: {accuracy:.6f}.'\n",
    "    print(content)\n",
    "\n",
    "    if accuracy > accuracy_max:\n",
    "        print(f'accuracy ({accuracy_max:.6f} --> {accuracy:.6f}). Saving model ...')\n",
    "        torch.save(model.state_dict(), f'{model_dir}dense121_feature_extractor.pth')\n",
    "        accuracy_max = accuracy\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
